apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: test-matteo-storesomething
  annotations:
    tekton.dev/output_artifacts: '{"index-on-modelregistry": [{"key": "artifacts/$PIPELINERUN/index-on-modelregistry/saveartifact.tgz",
      "name": "index-on-modelregistry-saveartifact", "path": "/tmp/outputs/saveartifact/data"}],
      "train-model": [{"key": "artifacts/$PIPELINERUN/train-model/outgoingfile.tgz",
      "name": "train-model-outgoingfile", "path": "/tmp/outputs/outgoingfile/data"},
      {"key": "artifacts/$PIPELINERUN/train-model/outgoingfile2.tgz", "name": "train-model-outgoingfile2",
      "path": "/tmp/outputs/outgoingfile2/data"}], "validate-model": [{"key": "artifacts/$PIPELINERUN/validate-model/Output.tgz",
      "name": "validate-model-Output", "path": "/tmp/outputs/Output/data"}]}'
    tekton.dev/input_artifacts: '{"index-on-modelregistry": [{"name": "train-model-outgoingfile2",
      "parent_task": "train-model"}], "validate-model": [{"name": "train-model-outgoingfile",
      "parent_task": "train-model"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"index-on-modelregistry": [["saveartifact", "$(workspaces.index-on-modelregistry.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/saveartifact"]],
      "logg-env-function": [], "train-model": [["outgoingfile", "$(workspaces.train-model.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/outgoingfile"],
      ["outgoingfile2", "$(workspaces.train-model.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/outgoingfile2"]],
      "validate-model": [["Output", "$(results.Output.path)"]], "when-not-validated":
      []}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "I need to store on S3,
      I need to know which location in the bucket, I need to know the name of the
      S3 bucket", "inputs": [{"name": "my_input"}], "name": "Test Matteo storesomething"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  params:
  - name: my_input
    value: ''
  pipelineSpec:
    params:
    - name: my_input
    tasks:
    - name: logg-env-function
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def logg_env_function():
              import os
              import logging
              logging.basicConfig(level=logging.INFO)
              logging.info(os.environ)

            import argparse
            _parser = argparse.ArgumentParser(prog='Logg env function', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = logg_env_function(**_parsed_args)
          image: registry.access.redhat.com/ubi8/python-39
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Logg env function",
              "outputs": [], "version": "Logg env function@sha256=5805404fb4eacfffc911742202dbad8bfb601f7d6ab4310233474e2583c30537"}'
    - name: train-model
      params:
      - name: my_input
        value: $(params.my_input)
      - name: pipelineRun-uid
        value: $(context.pipelineRun.uid)
      taskSpec:
        steps:
        - name: main
          args:
          - --my-input
          - $(inputs.params.my_input)
          - --input2
          - $(params.pipelineRun-uid)
          - --outgoingfile
          - $(workspaces.train-model.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/outgoingfile
          - --outgoingfile2
          - $(workspaces.train-model.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/outgoingfile2
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def train_model(my_input, input2, outgoingfile, outgoingfile2):
                import os
                import shutil
                import pickle
                import json
                model = dict()
                model['my_input'] = my_input
                model['input2'] = input2
                model['osenviron'] = json.dumps(dict(os.environ))
                print(model)
                print(outgoingfile)

                import tensorflow as tf
                from tensorflow import keras
                from tensorflow.keras import Sequential
                (X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()

                X_train = X_train/255
                X_test = X_test/255
                print('y_train[7] is the label:', y_train[7])

                model = Sequential()
                model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), input_shape=(28, 28, 1)))
                model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))
                model.add(tf.keras.layers.Flatten())
                model.add(tf.keras.layers.Dense(units=64, activation=tf.nn.relu))
                model.add(tf.keras.layers.Dropout(rate=0.2))
                model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))
                model.compile(optimizer='adam',
                            loss='sparse_categorical_crossentropy',
                            metrics=['accuracy'])
                print(model.summary())
                history = model.fit(X_train,y_train,epochs=3)
                import tf2onnx
                import onnx
                input_signature = [tf.TensorSpec([1, 28, 28], tf.double, name='x')]
                onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=12)
                onnx.save(onnx_model, "model.onnx")

                def save_pickle(model, object_file):
                    with open(object_file, "wb") as f:
                        pickle.dump(model, f)

                save_pickle(model, outgoingfile)
                shutil.copyfile('model.onnx', outgoingfile2)

            import argparse
            _parser = argparse.ArgumentParser(prog='Train model', description='')
            _parser.add_argument("--my-input", dest="my_input", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input2", dest="input2", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--outgoingfile", dest="outgoingfile", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--outgoingfile2", dest="outgoingfile2", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = train_model(**_parsed_args)
          image: image-registry.openshift-image-registry.svc:5000/odh/jupyter-tensorflow-notebook:2023.1
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        - image: registry.access.redhat.com/ubi8/python-38
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: registry.access.redhat.com/ubi8/python-38
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.train-model.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/outgoingfile $(results.outgoingfile.path)
            copy_artifact $(workspaces.train-model.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/outgoingfile2 $(results.outgoingfile2.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: my_input
        - name: pipelineRun-uid
        results:
        - name: outgoingfile
          type: string
          description: /tmp/outputs/outgoingfile/data
        - name: outgoingfile2
          type: string
          description: /tmp/outputs/outgoingfile2/data
        - name: taskrun-name
          type: string
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            artifact_outputs: '["model_file", "onnx_file"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Train model",
              "outputs": [{"name": "outgoingfile", "type": "model_file"}, {"name":
              "outgoingfile2", "type": "onnx_file"}], "version": "Train model@sha256=03d6a4ad3d1b99fd960b9c354a5f95b06dd7ccf99c15ffce1107e8bd0f38dff9"}'
        workspaces:
        - name: train-model
      workspaces:
      - name: train-model
        workspace: test-matteo-storesomething
    - name: validate-model
      params:
      - name: train-model-trname
        value: $(tasks.train-model.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - --incomingfile
          - $(workspaces.validate-model.path)/artifacts/$ORIG_PR_NAME/$(params.train-model-trname)/outgoingfile
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def validate_model(incomingfile):
                # For demo purposes, always validate.
                return True

            def _serialize_bool(bool_value: bool) -> str:
                if isinstance(bool_value, str):
                    return bool_value
                if not isinstance(bool_value, bool):
                    raise TypeError('Value "{}" has type "{}" instead of bool.'.format(
                        str(bool_value), str(type(bool_value))))
                return str(bool_value)

            import argparse
            _parser = argparse.ArgumentParser(prog='Validate model', description='')
            _parser.add_argument("--incomingfile", dest="incomingfile", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = validate_model(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_bool,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: image-registry.openshift-image-registry.svc:5000/odh/jupyter-tensorflow-notebook:2023.1
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: train-model-trname
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Validate model",
              "outputs": [{"name": "Output", "type": "Boolean"}], "version": "Validate
              model@sha256=7fc28d45922ffef1f0b8f455c6471712179ef33c5c7bf435f80ecb4572cd2528"}'
        workspaces:
        - name: validate-model
      workspaces:
      - name: validate-model
        workspace: test-matteo-storesomething
      runAfter:
      - train-model
    - name: index-on-modelregistry
      params:
      - name: train-model-trname
        value: $(tasks.train-model.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - --incomingfile
          - $(workspaces.index-on-modelregistry.path)/artifacts/$ORIG_PR_NAME/$(params.train-model-trname)/outgoingfile2
          - --saveartifact
          - $(workspaces.index-on-modelregistry.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/saveartifact
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def index_on_ModelRegistry(incomingfile, saveartifact):
                from model_registry import ModelRegistry
                from datetime import datetime
                import onnx
                import boto3
                import os
                print(f'incomingfile: {incomingfile}')
                onnx_model = onnx.load(incomingfile)
                onnx.checker.check_model(onnx_model)
                registeredmodel_name = "mnist"
                version_name = "v"+datetime.now().strftime("%Y%m%d%H%M%S")
                print(f"Will be using: {registeredmodel_name}:{version_name} in the remainder of this task")

                s3 = boto3.resource(
                    service_name='s3',
                    region_name=os.environ['AWS_DEFAULT_REGION'],
                    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],
                    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],
                    use_ssl=False,
                    endpoint_url=os.environ['AWS_S3_ENDPOINT'],
                    verify=False
                )

                bucket_name = os.environ['AWS_S3_BUCKET']
                odh_secret_name = f'aws-connection-{bucket_name}'
                in_bucket_path = version_name
                in_bucket_target = f'{in_bucket_path}/mnist.onnx'
                full_bucket_target = f's3://{bucket_name}/{in_bucket_target}'

                my_bucket = s3.Bucket(bucket_name)
                my_bucket.upload_file(incomingfile, in_bucket_target)
                for obj in my_bucket.objects.filter():
                    print(obj.key)

                registry = ModelRegistry(server_address="modelregistry-sample", port=9090, author="author")

                rm = registry.register_model(registeredmodel_name,
                                                full_bucket_target,
                                                model_format_name="onnx",
                                                model_format_version="1",
                                                storage_key=odh_secret_name,
                                                storage_path=in_bucket_path,
                                                version=version_name,
                                                description="demo20231121 e2e MNIST",
                                                )
                print("RegisteredModel:")
                print(registry.get_registered_model(registeredmodel_name))
                print("ModelVersion:")
                print(registry.get_model_version(registeredmodel_name, version_name))
                print("ModelArtifact:")
                print(registry.get_model_artifact(registeredmodel_name, version_name))

                print('end.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Index on ModelRegistry', description='')
            _parser.add_argument("--incomingfile", dest="incomingfile", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--saveartifact", dest="saveartifact", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = index_on_ModelRegistry(**_parsed_args)
          env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: aws-connection-mybucket
                key: AWS_ACCESS_KEY_ID
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                name: aws-connection-mybucket
                key: AWS_DEFAULT_REGION
          - name: AWS_S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: aws-connection-mybucket
                key: AWS_S3_BUCKET
          - name: AWS_S3_ENDPOINT
            valueFrom:
              secretKeyRef:
                name: aws-connection-mybucket
                key: AWS_S3_ENDPOINT
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-connection-mybucket
                key: AWS_SECRET_ACCESS_KEY
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
          image: quay.io/mmortari/rdsp:latest
        - image: registry.access.redhat.com/ubi8/python-38
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: registry.access.redhat.com/ubi8/python-38
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.index-on-modelregistry.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/saveartifact $(results.saveartifact.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: train-model-trname
        results:
        - name: saveartifact
          type: string
          description: /tmp/outputs/saveartifact/data
        - name: taskrun-name
          type: string
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            artifact_outputs: '["saveartifact"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Index on ModelRegistry",
              "outputs": [{"name": "saveartifact", "type": "saveartifact"}], "version":
              "Index on ModelRegistry@sha256=0ef0abd0ee9d128916be98197111baeba3441d25cf6db877f52481c623e2a8b9"}'
        workspaces:
        - name: index-on-modelregistry
      when:
      - input: $(tasks.condition-1.results.outcome)
        operator: in
        values:
        - "true"
      workspaces:
      - name: index-on-modelregistry
        workspace: test-matteo-storesomething
      runAfter:
      - train-model
    - name: when-not-validated
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def when_not_validated():
                print("The model built by CI failed to match required Validation, hence won't be registered in Model Registry")

            import argparse
            _parser = argparse.ArgumentParser(prog='When not validated', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = when_not_validated(**_parsed_args)
          image: registry.access.redhat.com/ubi8/python-39
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "When not validated",
              "outputs": [], "version": "When not validated@sha256=d5467e85d71ddce3cbf508bf9c725e6e194b29bc8061a4faec4713edb2064750"}'
      when:
      - input: $(tasks.condition-2.results.outcome)
        operator: in
        values:
        - "true"
    - name: condition-1
      params:
      - name: operand1
        value: $(tasks.validate-model.results.Output)
      - name: operand2
        value: "True"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi8/python-38
    - name: condition-2
      params:
      - name: operand1
        value: $(tasks.validate-model.results.Output)
      - name: operand2
        value: "False"
      - name: operator
        value: ==
      taskSpec:
        results:
        - name: outcome
          type: string
          description: Conditional task outcome
        params:
        - name: operand1
        - name: operand2
        - name: operator
        steps:
        - name: main
          command:
          - sh
          - -ec
          - program_path=$(mktemp); printf "%s" "$0" > "$program_path";  python3 -u
            "$program_path" "$1" "$2"
          args:
          - |
            import sys
            input1=str.rstrip(sys.argv[1])
            input2=str.rstrip(sys.argv[2])
            try:
              input1=int(input1)
              input2=int(input2)
            except:
              input1=str(input1)
            outcome="true" if (input1 $(inputs.params.operator) input2) else "false"
            f = open("/tekton/results/outcome", "w")
            f.write(outcome)
            f.close()
          - $(inputs.params.operand1)
          - $(inputs.params.operand2)
          image: registry.access.redhat.com/ubi8/python-38
    workspaces:
    - name: test-matteo-storesomething
  workspaces:
  - name: test-matteo-storesomething
    volumeClaimTemplate:
      spec:
        storageClassName: standard-csi
        accessModes:
        - ReadWriteMany
        resources:
          requests:
            storage: 2Gi
