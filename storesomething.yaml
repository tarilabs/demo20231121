apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: test-matteo-storesomething
  annotations:
    tekton.dev/output_artifacts: '{"os-environ-print": [{"key": "artifacts/$PIPELINERUN/os-environ-print/model.tgz",
      "name": "os-environ-print-model", "path": "/tmp/outputs/model/data"}]}'
    tekton.dev/input_artifacts: '{}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"logg-env-function": [], "os-environ-print": [["model",
      "$(results.model.path)"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "I need to store on S3,
      I need to know which location in the bucket, I need to know the name of the
      S3 bucket", "inputs": [{"name": "my_input"}], "name": "Test Matteo storesomething"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  params:
  - name: my_input
    value: ''
  pipelineSpec:
    params:
    - name: my_input
    tasks:
    - name: logg-env-function
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def logg_env_function():
              import os
              import logging
              logging.basicConfig(level=logging.INFO)
              logging.info(os.environ)

            import argparse
            _parser = argparse.ArgumentParser(prog='Logg env function', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = logg_env_function(**_parsed_args)
          env:
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-connection-mybucket
                key: AWS_SECRET_ACCESS_KEY
          image: ubi8/python-39
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Logg env function",
              "outputs": [], "version": "Logg env function@sha256=2eb1f7435ad1758ad399c289eae48abebbb73f2a637837078149f5f0a9aba483"}'
    - name: os-environ-print
      params:
      - name: my_input
        value: $(params.my_input)
      - name: pipelineRun-uid
        value: $(context.pipelineRun.uid)
      taskSpec:
        steps:
        - name: main
          args:
          - --my-input
          - $(inputs.params.my_input)
          - --input2
          - $(params.pipelineRun-uid)
          - --model
          - $(results.model.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def os_environ_print(my_input, input2, model_file,):
                import os
                import pickle
                import json
                model = dict()
                model['my_input'] = my_input
                model['input2'] = input2
                model['osenviron'] = json.dumps(dict(os.environ))
                print(model)
                print(model_file)

                from tensorflow import keras
                (X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()

                X_train = X_train/255
                X_test = X_test/255
                print('y_train[7] is the label:', y_train[7])

                def save_pickle(object_file, target_object):
                    with open(object_file, "w") as f:
                        json.dump(target_object, f)

                save_pickle(model_file, model)

            import argparse
            _parser = argparse.ArgumentParser(prog='Os environ print', description='')
            _parser.add_argument("--my-input", dest="my_input", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input2", dest="input2", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model", dest="model_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = os_environ_print(**_parsed_args)
          image: quay.io/mmortari/rdsp:latest
        params:
        - name: my_input
        - name: pipelineRun-uid
        results:
        - name: model
          type: string
          description: /tmp/outputs/model/data
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            artifact_outputs: '["model_file"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Os environ print",
              "outputs": [{"name": "model", "type": "model_file"}], "version": "Os
              environ print@sha256=f658ea77394a6d275d3f69a2eb478c9d6415d80ef01f22c1e98d191e67d52259"}'
