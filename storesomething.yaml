apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: test-matteo-storesomething
  annotations:
    tekton.dev/output_artifacts: '{"step1": [{"key": "artifacts/$PIPELINERUN/step1/outgoingfile.tgz",
      "name": "step1-outgoingfile", "path": "/tmp/outputs/outgoingfile/data"}], "step2":
      [{"key": "artifacts/$PIPELINERUN/step2/saveartifact.tgz", "name": "step2-saveartifact",
      "path": "/tmp/outputs/saveartifact/data"}]}'
    tekton.dev/input_artifacts: '{"step2": [{"name": "step1-outgoingfile", "parent_task":
      "step1"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"logg-env-function": [], "step1": [["outgoingfile",
      "$(workspaces.step1.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/outgoingfile"]],
      "step2": [["saveartifact", "$(workspaces.step2.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/saveartifact"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "I need to store on S3,
      I need to know which location in the bucket, I need to know the name of the
      S3 bucket", "inputs": [{"name": "my_input"}], "name": "Test Matteo storesomething"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  params:
  - name: my_input
    value: ''
  pipelineSpec:
    params:
    - name: my_input
    tasks:
    - name: logg-env-function
      taskSpec:
        steps:
        - name: main
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def logg_env_function():
              import os
              import logging
              logging.basicConfig(level=logging.INFO)
              logging.info(os.environ)

            import argparse
            _parser = argparse.ArgumentParser(prog='Logg env function', description='')
            _parsed_args = vars(_parser.parse_args())

            _outputs = logg_env_function(**_parsed_args)
          env:
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-connection-mybucket
                key: AWS_SECRET_ACCESS_KEY
          image: registry.access.redhat.com/ubi8/python-39
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Logg env function",
              "outputs": [], "version": "Logg env function@sha256=5805404fb4eacfffc911742202dbad8bfb601f7d6ab4310233474e2583c30537"}'
    - name: step1
      params:
      - name: my_input
        value: $(params.my_input)
      - name: pipelineRun-uid
        value: $(context.pipelineRun.uid)
      taskSpec:
        steps:
        - name: main
          args:
          - --my-input
          - $(inputs.params.my_input)
          - --input2
          - $(params.pipelineRun-uid)
          - --outgoingfile
          - $(workspaces.step1.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/outgoingfile
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def step1(my_input, input2, outgoingfile):
                import os
                import pickle
                import json
                model = dict()
                model['my_input'] = my_input
                model['input2'] = input2
                model['osenviron'] = json.dumps(dict(os.environ))
                print(model)
                print(outgoingfile)

                from tensorflow import keras
                (X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()

                X_train = X_train/255
                X_test = X_test/255
                print('y_train[7] is the label:', y_train[7])

                def save_pickle(object_file, target_object):
                    with open(object_file, "w") as f:
                        json.dump(target_object, f)

                save_pickle(outgoingfile, model)

            import argparse
            _parser = argparse.ArgumentParser(prog='Step1', description='')
            _parser.add_argument("--my-input", dest="my_input", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input2", dest="input2", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--outgoingfile", dest="outgoingfile", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = step1(**_parsed_args)
          image: quay.io/mmortari/rdsp:latest
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        - image: registry.access.redhat.com/ubi8/python-38
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: registry.access.redhat.com/ubi8/python-38
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.step1.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/outgoingfile $(results.outgoingfile.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: my_input
        - name: pipelineRun-uid
        results:
        - name: outgoingfile
          type: string
          description: /tmp/outputs/outgoingfile/data
        - name: taskrun-name
          type: string
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            artifact_outputs: '["model_file"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Step1", "outputs":
              [{"name": "outgoingfile", "type": "model_file"}], "version": "Step1@sha256=75b5841a8b006c40b2208ecc4425bd5e485245c30a2bdd58f33c47e97df814e9"}'
        workspaces:
        - name: step1
      workspaces:
      - name: step1
        workspace: test-matteo-storesomething
    - name: step2
      params:
      - name: step1-trname
        value: $(tasks.step1.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - --incomingfile
          - $(workspaces.step2.path)/artifacts/$ORIG_PR_NAME/$(params.step1-trname)/outgoingfile
          - --saveartifact
          - $(workspaces.step2.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/saveartifact
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def _make_parent_dirs_and_return_path(file_path: str):
                import os
                os.makedirs(os.path.dirname(file_path), exist_ok=True)
                return file_path

            def step2(incomingfile, saveartifact):
                import shutil
                print(f'incomingfile: {incomingfile}')
                with open(incomingfile, 'r') as reader:
                    for line in reader:
                        print(line)
                shutil.copyfile(incomingfile, saveartifact)
                print('end.')

            import argparse
            _parser = argparse.ArgumentParser(prog='Step2', description='')
            _parser.add_argument("--incomingfile", dest="incomingfile", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--saveartifact", dest="saveartifact", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = step2(**_parsed_args)
          image: quay.io/mmortari/rdsp:latest
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        - image: registry.access.redhat.com/ubi8/python-38
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: registry.access.redhat.com/ubi8/python-38
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.step2.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/saveartifact $(results.saveartifact.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: step1-trname
        results:
        - name: saveartifact
          type: string
          description: /tmp/outputs/saveartifact/data
        - name: taskrun-name
          type: string
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            artifact_outputs: '["saveartifact"]'
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Step2", "outputs":
              [{"name": "saveartifact", "type": "saveartifact"}], "version": "Step2@sha256=72d19ba2010d12705983616126517ecd511691ca1486724a77c7de4e7f2b9df3"}'
        workspaces:
        - name: step2
      workspaces:
      - name: step2
        workspace: test-matteo-storesomething
      runAfter:
      - step1
    workspaces:
    - name: test-matteo-storesomething
  workspaces:
  - name: test-matteo-storesomething
    volumeClaimTemplate:
      spec:
        storageClassName: standard-csi
        accessModes:
        - ReadWriteMany
        resources:
          requests:
            storage: 2Gi
